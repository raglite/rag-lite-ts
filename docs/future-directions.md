# Future Directions for RAG-lite-ts

*Roadmap for extending multimodal capabilities and architectural evolution*

## Table of Contents

- [Overview](#overview)
- [Audio Support Roadmap](#audio-support-roadmap)
- [True Tri-Modal Architecture](#true-tri-modal-architecture)
- [Advanced Features](#advanced-features)
- [Technical Considerations](#technical-considerations)
- [Timeline and Priorities](#timeline-and-priorities)

## Overview

RAG-lite-ts currently supports two operational modes:

- **Text Mode**: Optimized for text-only content using sentence-transformers
- **Multimodal Vision Mode**: True multimodal support for text and images using CLIP

This document outlines future directions for extending the system's capabilities, with a focus on audio support and advanced multimodal features.

### Current State

```text
‚úÖ Text Mode: Text-only content (.md, .txt, .pdf, .docx)
‚úÖ Multimodal Vision Mode: Text + Images in unified space (CLIP)
üîÑ Audio Support: Planned with CLAP + Whisper
üîÆ Tri-Modal: Future unified text + image + audio
```

### Architectural Philosophy

**Maintain clean separation of concerns:**
- **Text Mode**: Text-only content ‚Üí Sentence-transformer embeddings
- **Multimodal Modes**: Multiple content types ‚Üí Unified embedding spaces
- **No hybrid approaches**: Each mode has a clear, consistent strategy

**Key Principle**: Don't compromise clean architecture for marginal features. Each mode should have a clear purpose and consistent behavior.

---

## Audio Support Roadmap

### Phase 1: Multimodal Audio Mode (CLAP + Whisper)

**Status**: Waiting for transformers.js support  
**Complexity**: Medium-High  
**Time to Market**: Weeks (after CLAP availability)  
**Dependencies**: CLAP models in transformers.js, Whisper (already available)

#### Description

Implement true multimodal audio support using **both CLAP and Whisper** together:
- **CLAP**: For audio-text unified embedding space (similarity search)
- **Whisper**: For speech transcription (metadata, display, exact quotes)

This dual-model approach provides the best of both worlds: semantic audio search via CLAP embeddings and searchable transcriptions via Whisper.

#### Architecture

```text
Text Input ‚Üí CLAP Text Encoder ‚Üí Text Embedding (512-dim)
                                        ‚Üì
                                  Unified Space
                                        ‚Üì
Audio Input ‚Üí CLAP Audio Encoder ‚Üí Audio Embedding (512-dim)
            ‚Üì
            Whisper Transcriber ‚Üí Transcription (metadata)
```

**Key Insight**: CLAP embeddings and Whisper transcriptions serve different purposes:
- **CLAP embeddings**: Vector search for audio similarity
- **Whisper transcriptions**: Metadata for display, context, and exact-match search

#### Why Both Models?

**CLAP Alone (Insufficient)**
```
‚úì Cross-modal search (text finds audio, audio finds text)
‚úì Audio similarity search
‚úì Works for music and non-speech audio
‚úó No transcriptions available
‚úó Can't search by exact quotes
‚úó No text for display/context
```

**Whisper Alone (Insufficient)**
```
‚úì Accurate transcriptions
‚úì Exact quote search
‚úì Text for display
‚úó No audio similarity search
‚úó Only works for speech (not music/sounds)
‚úó Not truly multimodal (no unified space)
```

**CLAP + Whisper Together (Optimal)**
```
‚úì Cross-modal search via CLAP
‚úì Audio similarity via CLAP
‚úì Exact quote search via transcriptions
‚úì Rich metadata for display
‚úì Works for speech AND non-speech audio
‚úì Best user experience
```

#### Implementation Details

```typescript
// New mode type
mode: 'multimodal-audio'

// Model registry additions
'laion/clap-htsat-unfused': {
  type: 'clap',
  dimensions: 512,
  supportedContentTypes: ['text', 'audio'],
  capabilities: {
    supportsText: true,
    supportsAudio: true,
    supportsCrossModalSearch: true,
    unifiedEmbeddingSpace: true
  }
}

// Dual-model audio processor
class AudioProcessor {
  private clap: CLAPEmbedder;
  private whisper: WhisperTranscriber;
  
  async processAudio(audioPath: string): Promise<AudioResult> {
    // Run in parallel for efficiency
    const [embedding, transcription] = await Promise.all([
      this.clap.embedAudio(audioPath),      // For similarity search
      this.whisper.transcribe(audioPath)     // For metadata/display
    ]);
    
    return {
      embedding: embedding.vector,           // Used for vector search
      contentType: 'audio',
      content: transcription.text,           // For display
      metadata: {
        transcription: transcription.text,
        timestamps: transcription.timestamps,
        language: transcription.language,
        duration: transcription.duration,
        confidence: transcription.confidence
      }
    };
  }
}
```

#### Configuration

```typescript
// raglite.config.js
export const config = {
  mode: 'multimodal-audio',
  embeddingModel: 'laion/clap-htsat-unfused',
  
  audio: {
    // CLAP settings
    enableCrossModalSearch: true,
    audioSampleRate: 48000,
    maxAudioDuration: 600,  // 10 minutes
    
    // Whisper settings
    transcriptionModel: 'Xenova/whisper-base',
    includeTimestamps: true,
    detectLanguage: true,
    chunkLongAudio: true
  },
  
  // Processing settings
  batchSize: 4,  // Conservative for audio
  rerankingStrategy: 'text-derived'  // Uses transcriptions
};
```

#### Use Cases

**Speech Content (Podcasts, Lectures, Interviews)**
```typescript
// Semantic search via CLAP
search("machine learning discussion")
‚Üí Finds audio with ML content (via CLAP embeddings)

// Exact quote search via transcription
search("neural networks are inspired by the brain")
‚Üí Finds exact quote in transcription

// Display results with context
{
  content: "Neural networks are inspired by the brain...",
  audioPath: "podcast-ep-42.mp3",
  timestamp: 145.2,  // Jump to 2:25 in audio
  duration: 3600
}
```

**Music and Sound Effects**
```typescript
// Audio similarity via CLAP
search("upbeat electronic music")
‚Üí Finds similar sounding music (CLAP handles non-speech)

// Cross-modal search
search("ocean waves crashing")
‚Üí Finds audio with ocean sounds (via CLAP semantic understanding)
```

#### Advantages

- ‚úÖ **True multimodal** - Unified CLAP embedding space
- ‚úÖ **Cross-modal search** - Text queries find audio, audio queries find text
- ‚úÖ **Audio similarity** - Find similar sounding audio clips
- ‚úÖ **Exact quotes** - Search transcriptions for specific words
- ‚úÖ **Rich metadata** - Transcriptions, timestamps, language detection
- ‚úÖ **Works for all audio** - Speech, music, sound effects
- ‚úÖ **Better UX** - Display transcriptions in search results

#### Technical Challenges

**1. Dual Model Management**
```typescript
// Need to coordinate two models
- CLAP: Audio embeddings (always required)
- Whisper: Transcriptions (optional but recommended)

// Resource management
- Load both models efficiently
- Parallel processing where possible
- Cleanup both models properly
```

**2. Audio Preprocessing**
```typescript
interface AudioPreprocessor {
  // Load and decode audio
  loadAudio(path: string): Promise<AudioBuffer>
  
  // Resample to model's expected rate
  resample(audio: AudioBuffer, targetRate: number): Promise<AudioBuffer>
  
  // Convert to mel spectrogram (if needed)
  toMelSpectrogram(audio: AudioBuffer): Promise<Float32Array>
  
  // Chunk long audio files
  chunkAudio(audio: AudioBuffer, chunkDuration: number): Promise<AudioBuffer[]>
  
  // Extract audio metadata
  extractMetadata(path: string): Promise<AudioMetadata>
}
```

**3. Storage Strategy**
```typescript
// Database schema
CREATE TABLE chunks (
  chunk_id TEXT PRIMARY KEY,
  content TEXT,              -- Transcription text (for display)
  content_type TEXT,         -- 'audio'
  metadata JSON              -- Full transcription + timestamps
);

// Vector index
// CLAP audio embeddings (512-dim)
// Used for similarity search

// Storage considerations
- Audio files are large (1-10MB+ each)
- Store original audio or compressed version?
- Filesystem vs database storage
- Efficient retrieval for playback
```

**4. Memory Management**
```typescript
// Audio processing is memory-intensive
class AudioBatchProcessor {
  private maxConcurrentAudio = 2;  // Much lower than text/images
  private audioMemoryLimit = 500;  // MB
  
  async processBatch(audioFiles: string[]): Promise<EmbeddingResult[]> {
    // Process in small batches
    // Release memory aggressively
    // Monitor memory usage closely
  }
}
```

**5. Transformers.js Compatibility**

**Critical Requirement**: Verify CLAP model availability in transformers.js

```typescript
// Validation needed before implementation
async function validateCLAPSupport(): Promise<boolean> {
  try {
    const { AutoModel } = await import('@huggingface/transformers');
    const model = await AutoModel.from_pretrained('laion/clap-htsat-unfused');
    return true;
  } catch (error) {
    console.warn('CLAP models not yet supported in transformers.js');
    return false;
  }
}
```

**If CLAP not available**: Implementation must wait for transformers.js support. Do not implement workarounds (Python backend, external APIs) as this violates the local-first principle.

---

### Phase 2: Audio Classification Enhancement (Wav2Vec2)

**Status**: Future consideration  
**Complexity**: Medium  
**Time to Market**: Months (after Phase 1)  
**Dependencies**: Fine-tuned Wav2Vec2 models in transformers.js

#### Description

**Optional enhancement** to Phase 1 that adds audio classification and metadata enrichment using Wav2Vec2. This is **not a replacement** for CLAP or Whisper, but an **additional layer** for richer metadata.

#### When to Add Wav2Vec2

Add Wav2Vec2 **only if** it provides one of these:

**‚úÖ Scenario 1: Better Transcription**
- Fine-tuned Wav2Vec2 is faster/more accurate than Whisper
- Better at handling specific domains (medical, legal, technical)
- Better accent/dialect support
- Lower resource usage

**Then**: Offer as **alternative to Whisper**
```typescript
audio: {
  transcriptionModel: 'whisper-base' | 'wav2vec2-large-960h',
  // User chooses based on their needs
}
```

**‚úÖ Scenario 2: Audio Classification/Tagging**
- Fine-tuned for audio event detection
- Speaker emotion (happy, sad, angry, excited)
- Audio scene classification (office, street, nature, music)
- Speaker characteristics (gender, age, accent)
- Audio quality metrics (noisy, clear, echo)

**Then**: Use for **metadata enrichment**
```typescript
{
  embedding: clapEmbedding,        // CLAP for similarity
  transcription: whisperText,      // Whisper for text
  metadata: {
    emotion: 'excited',            // Wav2Vec2 classification
    scene: 'outdoor',
    speakerCount: 2,
    audioQuality: 'clear',
    language: 'en',
    accent: 'american'
  }
}
```

**‚úÖ Scenario 3: Specialized Embeddings**
- Captures aspects CLAP doesn't (prosody, speaker identity, acoustic quality)
- Proven to improve search quality in testing
- Can be combined with CLAP embeddings

**Then**: Use for **hybrid embeddings**
```typescript
// Weighted combination
finalEmbedding = 0.7 * clapEmbedding + 0.3 * wav2vec2Embedding
```

#### When NOT to Add Wav2Vec2

**‚ùå Just another audio embedding model**
- Don't add if it's redundant with CLAP
- Don't add if it's not clearly better
- Don't add "just because it exists"

**‚ùå Requires significant fine-tuning**
- Goes against "works out of the box" philosophy
- Users shouldn't need ML expertise
- Managing fine-tuned models is complex

**‚ùå Not in transformers.js**
- Breaks local-first, no-backend principle
- Would require Python backend or external API
- Inconsistent with other models

#### Implementation (If Conditions Met)

```typescript
// Extensible audio processor interface
interface AudioProcessor {
  // Required: For similarity search
  embedAudio(path: string): Promise<EmbeddingResult>
  
  // Optional: For transcription
  transcribe?(path: string): Promise<TranscriptionResult>
  
  // Optional: For classification/tagging
  classify?(path: string): Promise<ClassificationResult>
}

// Phase 1: CLAP + Whisper
class StandardAudioProcessor implements AudioProcessor {
  private clap: CLAPEmbedder;
  private whisper: WhisperTranscriber;
  
  async embedAudio(path: string) {
    return this.clap.embedAudio(path);
  }
  
  async transcribe(path: string) {
    return this.whisper.transcribe(path);
  }
}

// Phase 2: Add Wav2Vec2 (if valuable)
class EnhancedAudioProcessor implements AudioProcessor {
  private clap: CLAPEmbedder;
  private whisper: WhisperTranscriber;
  private wav2vec2: Wav2Vec2Classifier;  // NEW
  
  async embedAudio(path: string) {
    return this.clap.embedAudio(path);  // Still CLAP for embeddings
  }
  
  async transcribe(path: string) {
    return this.whisper.transcribe(path);  // Still Whisper for text
  }
  
  async classify(path: string) {
    return this.wav2vec2.classify(path);  // NEW: Audio classification
  }
}
```

#### Practical Example

**Good use case for Wav2Vec2:**
```typescript
// User searches: "excited discussion about AI"
// 
// CLAP: Finds audio with "AI" semantic content
// Wav2Vec2: Filters for "excited" emotion
// Whisper: Confirms "discussion" in transcription
//
// Result: More precise search results
```

**Bad use case for Wav2Vec2:**
```typescript
// User searches: "jazz music"
//
// CLAP: Already finds jazz music perfectly
// Wav2Vec2: Provides redundant audio embedding
//
// Result: No improvement, just complexity
```

#### Decision Framework

Add Wav2Vec2 only if:
1. ‚úÖ It solves a problem CLAP + Whisper don't solve
2. ‚úÖ It's available in transformers.js
3. ‚úÖ It works out-of-the-box (no fine-tuning required)
4. ‚úÖ It provides clear, measurable value to users
5. ‚úÖ It doesn't compromise architectural simplicity

**Most likely scenario**: Wav2Vec2 would be useful for **audio classification/metadata enrichment**, not as a replacement for CLAP or Whisper.

---

### Phase 3: Federated Multi-Modal Search

**Status**: Future research  
**Complexity**: High  
**Time to Market**: Months  
**Dependencies**: Phase 1 complete

#### Description

Implement federated search across multiple specialized indexes, allowing users to search across text, images, and audio simultaneously while maintaining separate optimized indexes for each modality.

#### Architecture

```text
User Query (text/image/audio)
    ‚Üì
Query Router
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Text Index  ‚îÇ Vision Index‚îÇ Audio Index ‚îÇ
‚îÇ (Sentence-  ‚îÇ (CLIP)      ‚îÇ (CLAP)      ‚îÇ
‚îÇ Transformer)‚îÇ             ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì           ‚Üì             ‚Üì
Result Merger & Re-Ranker
    ‚Üì
Unified Results
```

#### Implementation Details

```typescript
class FederatedSearchEngine {
  private textIndex: SearchEngine
  private visionIndex: SearchEngine
  private audioIndex: SearchEngine
  
  async search(query: string | Buffer, options: SearchOptions) {
    // Route query to appropriate indexes
    const queryType = this.detectQueryType(query)
    
    // Search relevant indexes in parallel
    const results = await Promise.all([
      this.searchText(query, queryType),
      this.searchVision(query, queryType),
      this.searchAudio(query, queryType)
    ])
    
    // Merge and re-rank results
    return this.mergeResults(results, options)
  }
  
  private mergeResults(results: SearchResult[][], options: SearchOptions) {
    // Normalize scores across different embedding spaces
    // Apply cross-modal re-ranking
    // Return unified result set
  }
}
```

#### Advantages

- **Flexibility** - Each modality optimized independently
- **Scalability** - Indexes can be distributed
- **Specialization** - Best model for each content type
- **Backward compatible** - Works with existing indexes

#### Challenges

- **Score normalization** - Different embedding spaces have different scales
- **Cross-modal ranking** - How to compare text vs image vs audio relevance?
- **Query routing** - Determining which indexes to search
- **Result merging** - Combining results from different spaces

---

## True Tri-Modal Architecture

### Phase 4: Unified Tri-Modal Embeddings

**Status**: Long-term research  
**Complexity**: Very High  
**Time to Market**: 1-2 years  
**Dependencies**: Model availability (ImageBind or similar)

#### Description

Implement a single unified embedding space for text, images, and audio using models like Meta's ImageBind. This would enable true cross-modal search across all three modalities simultaneously.

#### Architecture

```text
Text Input ‚îÄ‚îÄ‚îê
             ‚îú‚îÄ‚îÄ‚Üí ImageBind Encoder ‚Üí Unified Embedding (1024-dim)
Image Input ‚îÄ‚î§
             ‚îÇ
Audio Input ‚îÄ‚îò

All embeddings directly comparable in same space
```

#### Model Options

**ImageBind (Meta)**
```typescript
'facebook/imagebind-huge': {
  type: 'imagebind',
  dimensions: 1024,
  supportedContentTypes: ['text', 'image', 'audio', 'video'],
  capabilities: {
    supportsMultimodal: true,
    supportsCrossModalSearch: true,
    unifiedEmbeddingSpace: true,
    modalityCount: 4
  }
}
```

#### Use Cases

- ‚úÖ Universal content search across all modalities
- ‚úÖ Text query finds images, audio, and video
- ‚úÖ Image query finds related text, audio, and video
- ‚úÖ Audio query finds related text, images, and video
- ‚úÖ True semantic understanding across modalities

#### Advantages

- **Single embedding space** - All content types comparable
- **Maximum flexibility** - Any query type finds any content type
- **Simplified architecture** - One model, one index
- **Future-proof** - Easy to add new modalities

#### Challenges

- **Model availability** - Waiting for transformers.js support
- **Computational requirements** - Larger models, more memory
- **Complexity** - More sophisticated preprocessing needed
- **Quality tradeoffs** - Jack-of-all-trades vs specialized models

#### Consideration: Keep Specialized Models?

Even with ImageBind, might want to keep specialized models:
- **CLIP** for high-quality image search
- **CLAP** for high-quality audio search
- **Whisper** for accurate transcriptions

**Hybrid approach**: ImageBind for cross-modal, specialized models for within-modal search.

---

## Advanced Features

### Video Support

**Status**: Future consideration  
**Approach**: Frame extraction + audio track separation

```text
Video processing pipeline:

Video File
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Frame       ‚îÇ Audio Track ‚îÇ
‚îÇ Extraction  ‚îÇ Extraction  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì               ‚Üì
CLIP Vision     CLAP Audio
Embeddings      Embeddings
    ‚Üì               ‚Üì
Temporal Aggregation
    ‚Üì
Video Embedding
```

#### Implementation Considerations

- **Frame sampling** - Extract keyframes or sample at intervals
- **Audio extraction** - Separate and process audio track
- **Temporal modeling** - Aggregate frame embeddings over time
- **Storage efficiency** - Videos are large, need smart storage
- **Search granularity** - Scene-level vs frame-level search

#### With Whisper Integration

```text
Video File
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Frames      ‚îÇ Audio Track ‚îÇ Speech      ‚îÇ
‚îÇ (CLIP)      ‚îÇ (CLAP)      ‚îÇ (Whisper)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì               ‚Üì               ‚Üì
Visual          Audio           Subtitles/
Embeddings      Embeddings      Captions
    ‚Üì               ‚Üì               ‚Üì
        Unified Video Representation
```

### Document Understanding

**Status**: Ongoing improvement  
**Focus**: Better handling of complex documents

#### Enhanced PDF Processing

```text
Multi-modal PDF understanding:

PDF Document
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Text    ‚îÇ Images  ‚îÇ Tables  ‚îÇ
‚îÇ Extract ‚îÇ Extract ‚îÇ Extract ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì         ‚Üì         ‚Üì
Unified Document Representation
```

#### Features

- **Layout understanding** - Preserve document structure
- **Table extraction** - Structured data handling
- **Figure captioning** - Link images to text
- **Cross-reference resolution** - Connect related sections

### Semantic Chunking

**Status**: Research phase  
**Goal**: Smarter content chunking based on semantics

```typescript
// Current: Fixed-size chunking
Text ‚Üí [Chunk 1] [Chunk 2] [Chunk 3] ...

// Future: Semantic chunking
Text ‚Üí [Topic 1] [Topic 2] [Topic 3] ...
       (variable size based on semantic boundaries)
```

#### Approaches

- **Topic modeling** - Detect topic boundaries
- **Sentence embeddings** - Group semantically similar sentences
- **Discourse analysis** - Understand document structure
- **Hierarchical chunking** - Multi-level granularity

### Query Understanding

**Status**: Future enhancement  
**Goal**: Better interpret user intent

#### Features

- **Query expansion** - Add related terms
- **Intent detection** - Understand what user wants
- **Multi-modal query** - Accept text + image + audio queries
- **Conversational context** - Remember previous queries

---

## Technical Considerations

### Transformers.js Ecosystem

**Current Limitations**
- Not all models available in transformers.js
- Some models require ONNX conversion
- Performance varies by model

**Monitoring**
- Track transformers.js releases for new model support
- Test new models as they become available
- Contribute to transformers.js ecosystem

**Critical for Audio Support**
- CLAP availability is blocking for Phase 1
- Whisper already available (ready to use)
- Wav2Vec2 availability determines Phase 2 feasibility

### Performance Optimization

**Memory Management**
```typescript
// Aggressive cleanup for ML resources
- Multiple garbage collection passes
- Resource pooling for models
- Lazy loading of embedders
- Streaming processing for large files

// Audio-specific considerations
- Audio files are memory-intensive
- Process in smaller batches (2-4 items)
- Release memory between batches
- Monitor memory usage closely
```

**Batch Processing**
```typescript
// Optimize batch sizes by modality
- Text: 16-32 items per batch
- Images: 8-16 items per batch
- Audio: 2-4 items per batch (memory intensive)
```

**Caching Strategy**
```typescript
// Multi-level caching
- Model cache (disk)
- Embedding cache (memory/disk)
- Result cache (memory)
- Transcription cache (disk)
```

### Storage Architecture

**Current: SQLite + Binary Index**
```
SQLite Database: Metadata, chunks, documents, transcriptions
Binary Index: Vector embeddings (hnswlib)
```

**Future Considerations for Audio**
- **Separate audio storage** - Filesystem for large audio files
- **Compression** - Reduce storage footprint (MP3, Opus, AAC)
- **Transcription storage** - Efficient storage of text + timestamps
- **Distributed storage** - Scale beyond single machine
- **Cloud integration** - S3, Azure Blob, etc. (optional)

**Storage Strategy for Audio**
```typescript
interface AudioStorageConfig {
  // Store original or compressed?
  storeOriginal: boolean
  compressionFormat: 'mp3' | 'opus' | 'aac'
  compressionBitrate: number  // kbps
  
  // Chunking for long audio
  maxChunkDuration: number    // seconds
  chunkOverlap: number        // seconds
  
  // Storage location
  storageStrategy: 'filesystem' | 'database' | 'hybrid'
  
  // Transcription storage
  storeTranscriptions: boolean
  transcriptionFormat: 'json' | 'vtt' | 'srt'
}
```

### API Design

**Maintain Simplicity**
```typescript
// Simple API should remain simple
const search = new SearchEngine('./index.bin', './db.sqlite')
const results = await search.search('query')

// Advanced features opt-in
const search = new SearchEngine('./index.bin', './db.sqlite', {
  audioSupport: true,
  includeTranscriptions: true,
  audioClassification: true  // Phase 2
})
```

**Mode Detection**
```typescript
// Mode automatically detected from database
const search = new SearchEngine('./index.bin', './db.sqlite')
// Detects: text, multimodal-vision, or multimodal-audio

// Search works the same regardless of mode
const results = await search.search('query')
```

---

## Timeline and Priorities

### Immediate (Q1-Q2 2026)

**Wait for CLAP Support**
- Monitor transformers.js releases
- Test CLAP models as they become available
- Prepare audio preprocessing infrastructure
- Design audio storage strategy

**Preparation Work**
- Design audio processor interface
- Implement audio file validation
- Build audio preprocessing utilities
- Create test suite for audio processing

### Short-term (Q2-Q3 2026)

**Phase 1: Multimodal Audio Mode (CLAP + Whisper)**
- Depends on transformers.js CLAP support
- Implement CLAPEmbedder
- Integrate Whisper for transcriptions
- Add multimodal-audio mode
- Optimize audio preprocessing
- Performance testing and tuning
- Documentation and examples

**Enhanced Document Processing**
- Better PDF handling
- Table extraction
- Layout preservation
- Figure captioning

### Medium-term (Q4 2026 - Q1 2027)

**Phase 2: Audio Classification (Wav2Vec2)**
- Only if conditions are met (see Phase 2 section)
- Evaluate available Wav2Vec2 models
- Test classification accuracy
- Implement if valuable
- Integrate with Phase 1

**Phase 3: Federated Search**
- Implement query routing
- Build result merger
- Cross-modal re-ranking
- Performance optimization

**Video Support (Initial)**
- Frame extraction
- Audio track processing
- Basic video search

### Long-term (2027+)

**Phase 4: Unified Tri-Modal**
- Wait for ImageBind or similar in transformers.js
- Research and prototyping
- Architecture redesign if needed
- Migration path for existing users

**Advanced Features**
- Semantic chunking
- Query understanding
- Conversational search
- Real-time indexing

---

## Success Metrics

### Phase 1 Success Criteria (CLAP + Whisper)
- ‚úÖ Audio files can be ingested and searched
- ‚úÖ Cross-modal search works (text finds audio, audio finds text)
- ‚úÖ Transcriptions are accurate (>90% for clear speech)
- ‚úÖ Audio similarity search works reliably
- ‚úÖ Processing time acceptable (less than 2x real-time for transcription)
- ‚úÖ No breaking changes to existing API
- ‚úÖ Clear documentation and examples

### Phase 2 Success Criteria (Wav2Vec2)
- ‚úÖ Audio classification improves search quality (measurable)
- ‚úÖ Metadata enrichment provides user value
- ‚úÖ No significant performance degradation
- ‚úÖ Optional feature (doesn't break without it)
- ‚úÖ Clear use cases documented

### Overall Goals
- **Maintain simplicity** - Easy to use for common cases
- **Preserve performance** - No significant slowdown
- **Backward compatibility** - Existing code continues to work
- **Clear documentation** - Users understand capabilities and limitations
- **Production ready** - Reliable, tested, well-supported
- **Clean architecture** - No hybrid approaches or compromises

---

## Contributing

We welcome contributions to these future directions! Areas where community help would be valuable:

- **Model testing** - Test new models as they become available in transformers.js
- **Performance optimization** - Improve batch processing and memory management
- **Documentation** - Help document new features and use cases
- **Use case validation** - Share your use cases and requirements
- **Code contributions** - Implement features from this roadmap

See [CONTRIBUTING.md](../CONTRIBUTING.md) for guidelines.

---

## Conclusion

RAG-lite-ts has a clear path forward for expanding multimodal capabilities. The phased approach allows us to:

1. **Wait for ecosystem maturity** - CLAP support in transformers.js
2. **Deliver comprehensive audio support** (Phase 1) with CLAP + Whisper
3. **Optionally enhance** (Phase 2) with Wav2Vec2 if valuable
4. **Scale intelligently** (Phase 3) with federated search
5. **Future-proof** (Phase 4) with unified tri-modal embeddings

Each phase builds on the previous one while maintaining the core philosophy of RAG-lite-ts: **local-first, lightweight, and simple to use**.

### Key Principles

1. **Clean Architecture** - No hybrid approaches, each mode has clear purpose
2. **Wait for Ecosystem** - Don't compromise on local-first principle
3. **Dual Models When Valuable** - CLAP + Whisper together provide best UX
4. **Optional Enhancements** - Wav2Vec2 only if it adds clear value
5. **Maintain Simplicity** - Advanced features should be opt-in

The key is to wait for the right tools (CLAP in transformers.js), then implement comprehensive audio support that provides real value to users, rather than rushing to ship something that compromises the architecture or user experience.

---

*Last Updated: October 2025*  
*Status: Living document - will be updated as plans evolve*
